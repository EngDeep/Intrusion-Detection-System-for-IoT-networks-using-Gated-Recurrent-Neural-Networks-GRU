{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing Started.....\n",
      "\n",
      "Original number of records in the training dataset before removing duplicates is:  7000\n",
      "Number of records in the training dataset after removing the duplicates is : 7000 \n",
      "\n",
      "Data Preprocessing Started.....\n",
      "\n",
      "Original number of records in the testing dataset before removing duplicates is:  6200\n",
      "Number of records in the training dataset after removing the duplicates is : 6200 \n",
      "\n",
      "0 Cost for this epoch is 5.81112\n",
      "1 Cost for this epoch is 5.44382\n",
      "2 Cost for this epoch is 5.18188\n",
      "3 Cost for this epoch is 4.92499\n",
      "4 Cost for this epoch is 4.68146\n",
      "5 Cost for this epoch is 4.44261\n",
      "6 Cost for this epoch is 4.21419\n",
      "7 Cost for this epoch is 4.00923\n",
      "8 Cost for this epoch is 3.80604\n",
      "9 Cost for this epoch is 3.60479\n",
      "Accuracy 0.632258\n",
      "test Output is : [[  0.2971884   -1.05368376]\n",
      " [ -2.20881653   3.1637845 ]\n",
      " [  3.97210217  -0.27501708]\n",
      " ..., \n",
      " [  1.28061664   6.72052193]\n",
      " [  2.19289112  -2.11706829]\n",
      " [  0.83300078  10.16396809]]\n",
      "test labels are : [[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " ..., \n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "confusion matrix \n",
      " [[ 753 1399]\n",
      " [ 881 3167]]\n",
      "new accuracy 0.632258\n",
      "Accuracy calculated through confusion matrix 0.632258064516\n",
      "Precision \n",
      " 0.460832313341\n",
      "Recall (DR)\n",
      " 0.349907063197\n",
      "F1 Score is \n",
      " 0.397781299525\n",
      "False Alarm Rate is \n",
      " 0.217638339921\n",
      "Efficincy is \n",
      " 1.60774550718\n"
     ]
    }
   ],
   "source": [
    "#Importing all the required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Code Snippet remove the duplicates from the intrusion detection dataset\n",
    "print (\"Data Preprocessing Started.....\\n\")\n",
    "traindataframe = pd.read_csv(\"C:/Users/manp/Documents/IoT/Final/ApplicationLayer/ApplicationLayerTrainingData1.txt\",header = None,engine = 'python',sep=\",\")\n",
    "recordcount = len(traindataframe)\n",
    "print (\"Original number of records in the training dataset before removing duplicates is: \" , recordcount)\n",
    "traindataframe.drop_duplicates(subset=None, inplace=True)  # Python command to drop duplicates\n",
    "newrecordcount = len(traindataframe)\n",
    "print (\"Number of records in the training dataset after removing the duplicates is :\", newrecordcount,\"\\n\")\n",
    "#Dropping the labels to a different dataset which is used to train the recurrent neural network classifier\n",
    "train_X = traindataframe.drop(traindataframe.columns[41],axis=1)\n",
    "labels = traindataframe.drop(traindataframe.columns[0:41],axis=1)\n",
    "\n",
    "\n",
    "# Convert Categorial data to the numerical data for the efficient classification\n",
    "train_X[train_X.columns[1:4]] = train_X[train_X.columns[1:4]].stack().rank(method='dense').unstack()\n",
    "\n",
    "#labels.replace(['normal.',!'normal.'],[1,0],inplace = True)\n",
    "labels[labels[41]!='normal.'] = 0\n",
    "labels[labels[41]=='normal.'] = 1\n",
    "#print (labels[41].value_counts())\n",
    "\n",
    "inputX = train_X.loc[:,train_X.columns[0:41]].astype(float)\n",
    "\n",
    "labels.columns = [\"y1\"]\n",
    "#labels.loc[:,('y2')] = [0,1,1,1,0,1,0,0,0,]\n",
    "\n",
    "labels.loc[:,('y2')]=labels['y1'] ==0\n",
    "labels.loc[:,('y2')] = labels['y2'].astype(int)\n",
    "inputY = labels.as_matrix()\n",
    "inputX = inputX.as_matrix()\n",
    "\n",
    "\n",
    "# Code Snippet for test data\n",
    "print (\"Data Preprocessing Started.....\\n\")\n",
    "testdataframe = pd.read_csv(\"C:/Users/manp/Documents/IoT/Final/ApplicationLayer/ApplicationLayerTestData1.txt\",header = None,engine = 'python',sep=\",\")\n",
    "testrecordcount = len(testdataframe)\n",
    "print (\"Original number of records in the testing dataset before removing duplicates is: \" , testrecordcount)\n",
    "testdataframe.drop_duplicates(subset=None, inplace=True)  # Python command to drop duplicates\n",
    "newtestrecordcount = len(testdataframe)\n",
    "print (\"Number of records in the training dataset after removing the duplicates is :\", newtestrecordcount,\"\\n\")\n",
    "#Dropping the labels to a different dataset which is used to train the recurrent neural network classifier\n",
    "test_X = testdataframe.drop(testdataframe.columns[41],axis=1)\n",
    "testlabels = testdataframe.drop(testdataframe.columns[0:41],axis=1)\n",
    "\n",
    "\n",
    "# Convert Categorial data to the numerical data for the efficient classification\n",
    "test_X[test_X.columns[1:4]] = test_X[test_X.columns[1:4]].stack().rank(method='dense').unstack()\n",
    "\n",
    "#labels.replace(['normal.',!'normal.'],[1,0],inplace = True)\n",
    "testlabels[testlabels[41]!='normal'] = 0\n",
    "testlabels[testlabels[41]=='normal'] = 1\n",
    "#print (testlabels[41].value_counts())\n",
    "\n",
    "inputX_test = test_X.loc[:,test_X.columns[0:41]].astype(float)\n",
    "\n",
    "testlabels.columns = [\"y1\"]\n",
    "\n",
    "testlabels.loc[:,('y2')]=testlabels['y1'] ==0\n",
    "testlabels.loc[:,('y2')] = testlabels['y2'].astype(int)\n",
    "inputY_test = testlabels.as_matrix()\n",
    "inputX_test = inputX_test.as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "display_step =1\n",
    "num_layers = 1\n",
    "#Input Placeholders\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32,shape = [None,41], name = \"x-input\")\n",
    "    y = tf.placeholder(tf.float32, shape = [None,2],name = \"y-input\")\n",
    "#Weights and Biases\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = tf.Variable(tf.random_normal([41,2]))\n",
    "\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "#Model\n",
    "with tf.name_scope(\"splitx\"):\n",
    "    newx = tf.split(x,1,0)\n",
    "with tf.name_scope(\"multicellconfig\"):\n",
    "    multicell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(41) for _ in range (num_layers)], state_is_tuple=True)\n",
    "    \n",
    "with tf.variable_scope('mygrucell'):\n",
    "    outputs,states = tf.contrib.rnn.static_rnn(multicell,newx,dtype=tf.float32, scope = None)\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    output = tf.add(tf.matmul(outputs[-1],W),b)\n",
    "\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = output))\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y,1))\n",
    "    cast = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(cast)\n",
    "#create summary for the cost and accuracy\n",
    "tf.summary.scalar(\"cost\",cost)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_op = tf.summary.merge_all()\n",
    "logs_path = \"tmp/gru/test10\"\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(logs_path, graph = tf.get_default_graph())\n",
    "        for i in range (training_epochs):\n",
    "            _,summary = sess.run([optimizer,summary_op], feed_dict = {x:inputX, y:inputY})\n",
    "            writer.add_summary(summary,i)\n",
    "    \n",
    "            if (i) % display_step == 0:\n",
    "                print (i,\"Cost for this epoch is\",sess.run(cost, feed_dict ={x :inputX,y:inputY}))\n",
    "        print (\"Accuracy\",accuracy.eval(feed_dict = {x:inputX_test,y:inputY_test}))\n",
    "        print (\"test Output is :\", sess.run(output,feed_dict = {x:inputX_test, y:inputY_test}))\n",
    "        print (\"test labels are :\", sess.run(y,feed_dict = {x:inputX_test, y:inputY_test}))\n",
    "        pred_class = sess.run(tf.argmax(output,1),feed_dict = {x:inputX_test,y:inputY_test})\n",
    "        labels_class = sess.run(tf.argmax(y,1),feed_dict = {x:inputX_test,y:inputY_test})\n",
    "        conf = tf.contrib.metrics.confusion_matrix(labels_class,pred_class,dtype = tf.int32)\n",
    "        print (\"confusion matrix \\n\", sess.run(conf, feed_dict={x:inputX_test, y:inputY_test}))\n",
    "        n = tf.cast(labels_class,tf.int64)\n",
    "        newaccuracy = tf.contrib.metrics.accuracy(pred_class,n)\n",
    "        print (\"new accuracy\", sess.run (newaccuracy, feed_dict = {x:inputX_test, y:inputY_test}))\n",
    "        TP = conf[0,0]\n",
    "        FN = conf [0,1]\n",
    "        FP = conf[1,0]\n",
    "        TN = conf[1,1]\n",
    "        AccConf = (TP+TN)/(TP+FP+TN+FN)\n",
    "        print (\"Accuracy calculated through confusion matrix\", sess.run (AccConf, feed_dict = {x:inputX_test,y:inputY_test}))\n",
    "        # Precision\n",
    "        Precision = TP/(TP+FP)\n",
    "        print (\"Precision \\n\",sess.run(Precision,feed_dict ={x:inputX_test, y:inputY_test}))\n",
    "        #Recall\n",
    "        Recall = TP/(TP+FN)\n",
    "        print (\"Recall (DR)\\n\", sess.run(Recall,feed_dict={x:inputX_test,y:inputY_test}))\n",
    "        #F score\n",
    "        FScore = 2*((Precision*Recall)/(Precision+Recall))\n",
    "        print (\"F1 Score is \\n\",sess.run(FScore,{x:inputX_test, y:inputY_test}))\n",
    "        #False Alarm Rate\n",
    "        FAR = FP/(FP+TN)\n",
    "        print (\"False Alarm Rate is \\n\",sess.run(FAR,feed_dict ={x:inputX_test,y:inputY_test}))\n",
    "        #Efficiency\n",
    "        Efficiency = Recall/FAR\n",
    "        print(\"Efficincy is \\n\",sess.run(Efficiency,feed_dict = {x:inputX_test, y:inputY_test}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
